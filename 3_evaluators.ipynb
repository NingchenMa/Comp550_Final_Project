{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wmd\n",
        "!pip install summ-eval\n",
        "!pip install bert_score\n",
        "!pip install sacrebleu\n",
        "!pip install wmd\n",
        "!pip install sentencepiece\n",
        "!pip install .nmt_bleu\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SNdiO9JNRrR9",
        "outputId": "cac4dbd7-45a3-421c-c3a4-9ba38a869548"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wmd in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from wmd) (1.19.5)\n",
            "Requirement already satisfied: summ-eval in /usr/local/lib/python3.7/dist-packages (0.89)\n",
            "Requirement already satisfied: blanc in /usr/local/lib/python3.7/dist-packages (from summ-eval) (0.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from summ-eval) (7.1.2)\n",
            "Requirement already satisfied: moverscore in /usr/local/lib/python3.7/dist-packages (from summ-eval) (1.0.3)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (from summ-eval) (0.6.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from summ-eval) (2.6.3)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from summ-eval) (0.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from summ-eval) (1.4.1)\n",
            "Requirement already satisfied: transformers>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from summ-eval) (4.14.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from summ-eval) (3.2.5)\n",
            "Requirement already satisfied: pyemd==0.5.1 in /usr/local/lib/python3.7/dist-packages (from summ-eval) (0.5.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from summ-eval) (0.29.24)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from summ-eval) (5.4.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from summ-eval) (1.15.0)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.7/dist-packages (from summ-eval) (0.3.11)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.7/dist-packages (from summ-eval) (1.3.0)\n",
            "Requirement already satisfied: spacy>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from summ-eval) (2.2.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from summ-eval) (0.0.46)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from summ-eval) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from summ-eval) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (4.62.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.0->summ-eval) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->summ-eval) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->summ-eval) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->summ-eval) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->summ-eval) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->summ-eval) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->summ-eval) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->summ-eval) (2.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.2.0->summ-eval) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.2.0->summ-eval) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.2.0->summ-eval) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.2.0->summ-eval) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.2.0->summ-eval) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.2.0->summ-eval) (0.2.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=2.2.0->summ-eval) (3.0.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert-score->summ-eval) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from bert-score->summ-eval) (1.10.0+cu111)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from bert-score->summ-eval) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert-score->summ-eval) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert-score->summ-eval) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score->summ-eval) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score->summ-eval) (1.3.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from moverscore->summ-eval) (2.3.2)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.7/dist-packages (from moverscore->summ-eval) (3.7.4.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert->summ-eval) (1.20.24)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.24 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert->summ-eval) (1.23.24)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert->summ-eval) (0.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert->summ-eval) (0.10.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->summ-eval) (0.8.9)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu->summ-eval) (0.4.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->summ-eval) (1.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza->summ-eval) (3.17.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from stanza->summ-eval) (1.6.1)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.7/dist-packages (0.3.11)\n",
            "Requirement already satisfied: transformers>=3.0.0numpy in /usr/local/lib/python3.7/dist-packages (from bert_score) (4.14.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert_score) (21.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (4.62.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert_score) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from bert_score) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert_score) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert_score) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert_score) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->bert_score) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->bert_score) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (4.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (0.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert_score) (2019.12.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.0numpy->bert_score) (3.6.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert_score) (1.25.11)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0numpy->bert_score) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0numpy->bert_score) (7.1.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2.3.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.4.4)\n",
            "Requirement already satisfied: wmd in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from wmd) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "\u001b[31mERROR: Invalid requirement: '.nmt_bleu'\u001b[0m\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.16.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 24.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Installing collected packages: regex, nltk\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.5 regex-2021.11.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "regex"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bleu Metric"
      ],
      "metadata": {
        "id": "m6P9M-Mp2h4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool\n",
        "import gin\n",
        "import sacrebleu\n",
        "from summ_eval.metric import Metric\n",
        "import os"
      ],
      "metadata": {
        "id": "-EJhTzwZmZTw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_DESCRIPTION = \"\"\"\\\n",
        "BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another.\n",
        "Quality is considered to be the correspondence between a machine's output and that of a human: \"the closer a machine translation is to a professional human translation,\n",
        "the better it is\" – this is the central idea behind BLEU. BLEU was one of the first metrics to claim a high correlation with human judgements of quality, and\n",
        "remains one of the most popular automated and inexpensive metrics.\n",
        "Scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations.\n",
        "Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. Intelligibility or grammatical correctness\n",
        "are not taken into account[citation needed].\n",
        "BLEU's output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1\n",
        "representing more similar texts. Few human translations will attain a score of 1, since this would indicate that the candidate is identical to one of the\n",
        "reference translations. For this reason, it is not necessary to attain a score of 1. Because there are more opportunities to match, adding additional\n",
        "reference translations will increase the BLEU score.\n",
        "\"\"\"\n",
        "\n",
        "_KWARGS_DESCRIPTION = \"\"\"\n",
        "Computes BLEU score of translated segments against one or more references.\n",
        "Args:\n",
        "    predictions: list of translations to score.\n",
        "        Each translation should be tokenized into a list of tokens.\n",
        "    references: list of lists of references for each translation.\n",
        "        Each reference should be tokenized into a list of tokens.\n",
        "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
        "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
        "Returns:\n",
        "    'bleu': bleu score,\n",
        "    'precisions': geometric mean of n-gram precisions,\n",
        "    'brevity_penalty': brevity penalty,\n",
        "    'length_ratio': ratio of lengths,\n",
        "    'translation_length': translation_length,\n",
        "    'reference_length': reference_length\n",
        "Examples:\n",
        "    >>> predictions = [\n",
        "    ...     [\"hello\", \"there\", \"general\", \"kenobi\"],                             # tokenized prediction of the first sample\n",
        "    ...     [\"foo\", \"bar\", \"foobar\"]                                             # tokenized prediction of the second sample\n",
        "    ... ]\n",
        "    >>> references = [\n",
        "    ...     [[\"hello\", \"there\", \"general\", \"kenobi\"], [\"hello\", \"there\", \"!\"]],  # tokenized references for the first sample (2 references)\n",
        "    ...     [[\"foo\", \"bar\", \"foobar\"]]                                           # tokenized references for the second sample (1 reference)\n",
        "    ... ]\n",
        "    >>> bleu = datasets.load_metric(\"bleu\")\n",
        "    >>> results = bleu.compute(predictions=predictions, references=references)\n",
        "    >>> print(results[\"bleu\"])\n",
        "    1.0\n",
        "\"\"\"\n",
        "\n",
        "#@gin.configurable\n",
        "class BleuMetric(Metric):\n",
        "    def __init__(self, sent_smooth_method='exp', sent_smooth_value=None, sent_use_effective_order=True, \\\n",
        "       smooth_method='exp', smooth_value=None, force=False, lowercase=False, \\\n",
        "       use_effective_order=False, n_workers=24):\n",
        "        \"\"\"\n",
        "        BLEU metric\n",
        "\n",
        "        Args:\n",
        "                :param smooth_value: For 'floor' smoothing, the floor value to use.\n",
        "                :param use_effective_order: Account for references that are shorter than the largest n-gram.\n",
        "                :param force: Ignore data that looks already tokenized\n",
        "                :param lowercase: Lowercase the data\n",
        "                :param n_workers: number of processes to use if using multiprocessing\n",
        "                sent* parameters are the same but specify what is used for evaluate_example\n",
        "\n",
        "        \"\"\"\n",
        "        self.sent_smooth_method = sent_smooth_method\n",
        "        self.sent_smooth_value = sent_smooth_value\n",
        "        self.sent_use_effective_order = sent_use_effective_order\n",
        "        self.smooth_method = smooth_method\n",
        "        self.smooth_value = smooth_value\n",
        "        self.force = force\n",
        "        self.lowercase = lowercase\n",
        "        self.use_effective_order = use_effective_order\n",
        "        self.n_workers = n_workers\n",
        "\n",
        "    def evaluate_example(self, summary, reference):\n",
        "        #print(\"BLEU is intended as a corpus-level metric. Be careful!\")\n",
        "        if isinstance(reference, str):\n",
        "            reference = [reference]\n",
        "        score = sacrebleu.sentence_bleu(summary, reference, smooth_method=self.sent_smooth_method, \\\n",
        "             smooth_value=self.sent_smooth_value, use_effective_order=self.sent_use_effective_order)\n",
        "        score_dict = {\"bleu\" : score.score}\n",
        "        #return score_dict\n",
        "        #print(score.score)\n",
        "        return score.score\n",
        "\n",
        "    # def evaluate_batch(self, summaries, references, aggregate=True):\n",
        "    #     if aggregate:\n",
        "    #         if isinstance(references[0], str):\n",
        "    #             references = [references]\n",
        "    #         score = sacrebleu.corpus_bleu(summaries, references, smooth_method=self.smooth_method, \\\n",
        "    #            smooth_value=self.smooth_value, force=self.force, lowercase=self.lowercase, \\\n",
        "    #            use_effective_order=self.use_effective_order)\n",
        "    #         score_dict = {\"bleu\": score.score}\n",
        "    #     else:\n",
        "    #         p = Pool(processes=self.n_workers)\n",
        "    #         score_dict = p.starmap(self.evaluate_example, zip(summaries, references))\n",
        "    #         p.close()\n",
        "    #     return score_dict\n",
        "\n",
        "    # property\n",
        "    # def supports_multi_ref(self):\n",
        "    #     return True"
      ],
      "metadata": {
        "id": "Peq879YmmiXY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for index in range(50):\n",
        "#     # Absolute path of a file\n",
        "#     old_name = str(index)+\".dec\"\n",
        "#     new_name = \"pnbert_\"+str(index)+\".dec\"\n",
        "#     # Renaming the file\n",
        "#     os.rename(old_name, new_name)"
      ],
      "metadata": {
        "id": "3GCfMQ5buU3S"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bleu Scores"
      ],
      "metadata": {
        "id": "JSCttkza2Ry7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load summaries and references...\n",
        "\n",
        "from pathlib import Path \n",
        "bleu = BleuMetric() \n",
        "\n",
        "ref = []\n",
        "for index in range(50):\n",
        "  txt = Path(str(index)+'.ref').read_text()\n",
        "  ref.append(txt)\n",
        "\n",
        "loop_sum = []\n",
        "for index in range(50):\n",
        "  txt = Path(\"loop_sum_\"+str(index)+'.dec').read_text()\n",
        "  loop_sum.append(txt)\n",
        "\n",
        "match_sum = []\n",
        "for index in range(50):\n",
        "  txt = Path(\"match_sum_\"+str(index)+'.dec').read_text()\n",
        "  match_sum.append(txt)\n",
        "\n",
        "pgen=[]\n",
        "for index in range(50):\n",
        "  txt = Path(\"pgen_\"+str(index)+'.dec').read_text()\n",
        "  pgen.append(txt)\n",
        "  \n",
        "pnbert= []\n",
        "for index in range(50):\n",
        "  txt = Path(\"pnbert_\"+str(index)+'.dec').read_text()\n",
        "  pnbert.append(txt)"
      ],
      "metadata": {
        "id": "MH6ILR0dsdLD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate scores...\n",
        "\n",
        "loop_sum_bleu = []\n",
        "for index in range(50):\n",
        "  score = bleu.evaluate_example(loop_sum[index],ref[index])\n",
        "  loop_sum_bleu.append(score)\n",
        "  #print(score)\n",
        "\n",
        "match_sum_bleu = []\n",
        "for index in range(50):\n",
        "  score = bleu.evaluate_example(match_sum[index],ref[index])\n",
        "  match_sum_bleu.append(score)\n",
        "  #print(score)\n",
        "\n",
        "pgen_bleu = []\n",
        "for index in range(50):\n",
        "  score = bleu.evaluate_example(pgen[index],ref[index])\n",
        "  pgen_bleu.append(score)\n",
        "  #print(score)\n",
        "\n",
        "pnbert_bleu = []\n",
        "for index in range(50):\n",
        "  score = bleu.evaluate_example(pnbert[index],ref[index])\n",
        "  pnbert_bleu.append(score)\n",
        "  #print(score)"
      ],
      "metadata": {
        "id": "tKcOsny91ELP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wtite results to CSV files"
      ],
      "metadata": {
        "id": "mR1n4-RS3ipn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing pandas as pd \n",
        "import pandas as pd \n",
        "   \n",
        "# dictionary of lists \n",
        "dict = {'loop_sum': loop_sum_bleu, 'match_sum': match_sum_bleu, 'pgen': pgen_bleu, 'pnbert':pnbert_bleu} \n",
        "     \n",
        "df = pd.DataFrame(dict)\n",
        "  \n",
        "# saving the dataframe\n",
        "df.to_csv('bleu_scores.csv')"
      ],
      "metadata": {
        "id": "2rRDeVGYulHz"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chrf(++) metric\n",
        "\n"
      ],
      "metadata": {
        "id": "G4pWPKHv2mZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2021 The HuggingFace Datasets Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Chrf(++) metric as available in sacrebleu. \"\"\"\n",
        "import sacrebleu as scb\n",
        "from packaging import version\n",
        "from sacrebleu import CHRF\n",
        "\n",
        "import datasets\n",
        "\n",
        "_CITATION = \"\"\"\\\n",
        "@inproceedings{popovic-2015-chrf,\n",
        "    title = \"chr{F}: character n-gram {F}-score for automatic {MT} evaluation\",\n",
        "    author = \"Popovi{\\'c}, Maja\",\n",
        "    booktitle = \"Proceedings of the Tenth Workshop on Statistical Machine Translation\",\n",
        "    month = sep,\n",
        "    year = \"2015\",\n",
        "    address = \"Lisbon, Portugal\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://aclanthology.org/W15-3049\",\n",
        "    doi = \"10.18653/v1/W15-3049\",\n",
        "    pages = \"392--395\",\n",
        "}\n",
        "@inproceedings{popovic-2017-chrf,\n",
        "    title = \"chr{F}++: words helping character n-grams\",\n",
        "    author = \"Popovi{\\'c}, Maja\",\n",
        "    booktitle = \"Proceedings of the Second Conference on Machine Translation\",\n",
        "    month = sep,\n",
        "    year = \"2017\",\n",
        "    address = \"Copenhagen, Denmark\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://aclanthology.org/W17-4770\",\n",
        "    doi = \"10.18653/v1/W17-4770\",\n",
        "    pages = \"612--618\",\n",
        "}\n",
        "@inproceedings{post-2018-call,\n",
        "    title = \"A Call for Clarity in Reporting {BLEU} Scores\",\n",
        "    author = \"Post, Matt\",\n",
        "    booktitle = \"Proceedings of the Third Conference on Machine Translation: Research Papers\",\n",
        "    month = oct,\n",
        "    year = \"2018\",\n",
        "    address = \"Belgium, Brussels\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://www.aclweb.org/anthology/W18-6319\",\n",
        "    pages = \"186--191\",\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "_DESCRIPTION = \"\"\"\n",
        "ChrF and ChrF++ are two MT evaluation metrics. They both use the F-score statistic for character n-gram matches,\n",
        "and ChrF++ adds word n-grams as well which correlates more strongly with direct assessment. \n",
        "\"\"\"\n",
        "\n",
        "_KWARGS_DESCRIPTION = \"\"\"\n",
        "Produces ChrF(++) scores for hypotheses given reference translations.\n",
        "Args:\n",
        "    predictions: The system stream (a sequence of segments).\n",
        "    references: A list of one or more reference streams (each a sequence of segments).\n",
        "    char_order: Character n-gram order.\n",
        "    word_order: Word n-gram order. If equals to 2, the metric is referred to as chrF++.\n",
        "    beta: Determine the importance of recall w.r.t precision.\n",
        "    lowercase: Enable case-insensitivity.\n",
        "    whitespace: If `True`, include whitespaces when extracting character n-grams.\n",
        "    eps_smoothing: If `True`, applies epsilon smoothing similar\n",
        "    to reference chrF++.py, NLTK and Moses implementations. Otherwise,\n",
        "    it takes into account effective match order similar to sacreBLEU < 2.0.0.\n",
        "Returns:\n",
        "    'score': The chrF (chrF++) score,\n",
        "    'char_order': The character n-gram order,\n",
        "    'word_order': The word n-gram order. If equals to 2, the metric is referred to as chrF++,\n",
        "    'beta': Determine the importance of recall w.r.t precision\n",
        "Examples:\n",
        "    >>> prediction = [\"The relationship between Obama and Netanyahu is not exactly friendly.\"]\n",
        "    >>> reference = [[\"The ties between Obama and Netanyahu are not particularly friendly.\"]]\n",
        "    >>> chrf = datasets.load_metric(\"chrf\")\n",
        "    >>> results = chrf.compute(predictions=prediction, references=reference)\n",
        "    >>> print(results)\n",
        "    {'score': 61.576379378113785, 'char_order': 6, 'word_order': 0, 'beta': 2}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
        "class ChrF(datasets.Metric):\n",
        "    def _info(self):\n",
        "        if version.parse(scb.__version__) < version.parse(\"1.4.12\"):\n",
        "            raise ImportWarning(\n",
        "                \"To use `sacrebleu`, the module `sacrebleu>=1.4.12` is required, and the current version of `sacrebleu` doesn't match this condition.\\n\"\n",
        "                'You can install it with `pip install \"sacrebleu>=1.4.12\"`.'\n",
        "            )\n",
        "        return datasets.MetricInfo(\n",
        "            description=_DESCRIPTION,\n",
        "            citation=_CITATION,\n",
        "            homepage=\"https://github.com/mjpost/sacreBLEU#chrf--chrf\",\n",
        "            inputs_description=_KWARGS_DESCRIPTION,\n",
        "            features=datasets.Features(\n",
        "                {\n",
        "                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n",
        "                    \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),\n",
        "                }\n",
        "            ),\n",
        "            codebase_urls=[\"https://github.com/mjpost/sacreBLEU#chrf--chrf\"],\n",
        "            reference_urls=[\n",
        "                \"https://github.com/m-popovic/chrF\",\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    def _compute(\n",
        "        self,\n",
        "        predictions,\n",
        "        references,\n",
        "        char_order: int = CHRF.CHAR_ORDER,\n",
        "        word_order: int = CHRF.WORD_ORDER,\n",
        "        beta: int = CHRF.BETA,\n",
        "        lowercase: bool = False,\n",
        "        whitespace: bool = False,\n",
        "        eps_smoothing: bool = False,\n",
        "    ):\n",
        "        references_per_prediction = len(references[0])\n",
        "        if any(len(refs) != references_per_prediction for refs in references):\n",
        "            raise ValueError(\"Sacrebleu requires the same number of references for each prediction\")\n",
        "        transformed_references = [[refs[i] for refs in references] for i in range(references_per_prediction)]\n",
        "\n",
        "        sb_chrf = CHRF(char_order, word_order, beta, lowercase, whitespace, eps_smoothing)\n",
        "        output = sb_chrf.corpus_score(predictions, transformed_references)\n",
        "\n",
        "        # return {\n",
        "        #     \"score\": output.score,\n",
        "        #     \"char_order\": output.char_order,\n",
        "        #     \"word_order\": output.word_order,\n",
        "        #     \"beta\": output.beta,\n",
        "        # }\n",
        "\n",
        "        return output.score"
      ],
      "metadata": {
        "id": "ZYnLnPRq2e5C"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chrf Score"
      ],
      "metadata": {
        "id": "ECm12BzP3C2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C = ChrF()"
      ],
      "metadata": {
        "id": "tcxnO87F3Lhs"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate scores...\n",
        "\n",
        "loop_sum_chrf = []\n",
        "for index in range(50):\n",
        "  score = C._compute(loop_sum[index],ref[index])\n",
        "  loop_sum_chrf.append(score)\n",
        "  #print(score)\n",
        "\n",
        "match_sum_chrf = []\n",
        "for index in range(50):\n",
        "  score = C._compute(match_sum[index],ref[index])\n",
        "  match_sum_chrf.append(score)\n",
        "  #print(score)\n",
        "\n",
        "pgen_chrf = []\n",
        "for index in range(50):\n",
        "  score = C._compute(pgen[index],ref[index])\n",
        "  pgen_chrf.append(score)\n",
        "  #print(score)\n",
        "\n",
        "pnbert_chrf = []\n",
        "for index in range(50):\n",
        "  score = C._compute(pnbert[index],ref[index])\n",
        "  pnbert_chrf.append(score)\n",
        "  #print(score)"
      ],
      "metadata": {
        "id": "F_V6Y2pZ287b"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to files"
      ],
      "metadata": {
        "id": "GkUKIOla4YOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary of lists \n",
        "dict = {'loop_sum': loop_sum_chrf, 'match_sum': match_sum_chrf, 'pgen': pgen_chrf, 'pnbert':pnbert_chrf} \n",
        "     \n",
        "df = pd.DataFrame(dict)\n",
        "  \n",
        "# saving the dataframe\n",
        "df.to_csv('chrf_scores.csv')"
      ],
      "metadata": {
        "id": "F9xs7gds4a12"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meteor Metric"
      ],
      "metadata": {
        "id": "IxXbnTwb3str"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2020 The HuggingFace Datasets Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" METEOR metric. \"\"\"\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "from nltk.translate import meteor_score\n",
        "\n",
        "import datasets\n",
        "from datasets.config import importlib_metadata, version\n",
        "\n",
        "\n",
        "NLTK_VERSION = version.parse(importlib_metadata.version(\"nltk\"))\n",
        "if NLTK_VERSION >= version.Version(\"3.6.4\"):\n",
        "    from nltk import word_tokenize\n",
        "\n",
        "\n",
        "_CITATION = \"\"\"\\\n",
        "@inproceedings{banarjee2005,\n",
        "  title     = {{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments},\n",
        "  author    = {Banerjee, Satanjeev  and Lavie, Alon},\n",
        "  booktitle = {Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},\n",
        "  month     = jun,\n",
        "  year      = {2005},\n",
        "  address   = {Ann Arbor, Michigan},\n",
        "  publisher = {Association for Computational Linguistics},\n",
        "  url       = {https://www.aclweb.org/anthology/W05-0909},\n",
        "  pages     = {65--72},\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "METEOR, an automatic metric for machine translation evaluation\n",
        "that is based on a generalized concept of unigram matching between the\n",
        "machine-produced translation and human-produced reference translations.\n",
        "Unigrams can be matched based on their surface forms, stemmed forms,\n",
        "and meanings; furthermore, METEOR can be easily extended to include more\n",
        "advanced matching strategies. Once all generalized unigram matches\n",
        "between the two strings have been found, METEOR computes a score for\n",
        "this matching using a combination of unigram-precision, unigram-recall, and\n",
        "a measure of fragmentation that is designed to directly capture how\n",
        "well-ordered the matched words in the machine translation are in relation\n",
        "to the reference.\n",
        "METEOR gets an R correlation value of 0.347 with human evaluation on the Arabic\n",
        "data and 0.331 on the Chinese data. This is shown to be an improvement on\n",
        "using simply unigram-precision, unigram-recall and their harmonic F1\n",
        "combination.\n",
        "\"\"\"\n",
        "\n",
        "_KWARGS_DESCRIPTION = \"\"\"\n",
        "Computes METEOR score of translated segments against one or more references.\n",
        "Args:\n",
        "    predictions: list of predictions to score. Each prediction\n",
        "        should be a string with tokens separated by spaces.\n",
        "    references: list of reference for each prediction. Each\n",
        "        reference should be a string with tokens separated by spaces.\n",
        "    alpha: Parameter for controlling relative weights of precision and recall. default: 0.9\n",
        "    beta: Parameter for controlling shape of penalty as a function of fragmentation. default: 3\n",
        "    gamma: Relative weight assigned to fragmentation penalty. default: 0.5\n",
        "Returns:\n",
        "    'meteor': meteor score.\n",
        "Examples:\n",
        "    >>> meteor = datasets.load_metric('meteor')\n",
        "    >>> predictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\n",
        "    >>> references = [\"It is a guide to action that ensures that the military will forever heed Party commands\"]\n",
        "    >>> results = meteor.compute(predictions=predictions, references=references)\n",
        "    >>> print(round(results[\"meteor\"], 4))\n",
        "    0.6944\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
        "class Meteor(datasets.Metric):\n",
        "    def _info(self):\n",
        "        return datasets.MetricInfo(\n",
        "            description=_DESCRIPTION,\n",
        "            citation=_CITATION,\n",
        "            inputs_description=_KWARGS_DESCRIPTION,\n",
        "            features=datasets.Features(\n",
        "                {\n",
        "                    \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n",
        "                    \"references\": datasets.Value(\"string\", id=\"sequence\"),\n",
        "                }\n",
        "            ),\n",
        "            codebase_urls=[\"https://github.com/nltk/nltk/blob/develop/nltk/translate/meteor_score.py\"],\n",
        "            reference_urls=[\n",
        "                \"https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.meteor_score\",\n",
        "                \"https://en.wikipedia.org/wiki/METEOR\",\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    def _download_and_prepare(self, dl_manager):\n",
        "        import nltk\n",
        "\n",
        "        nltk.download(\"wordnet\")\n",
        "        if NLTK_VERSION >= version.Version(\"3.6.4\"):\n",
        "            nltk.download(\"punkt\")\n",
        "\n",
        "    def _compute(self, predictions, references, alpha=0.9, beta=3, gamma=0.5):\n",
        "        if NLTK_VERSION >= version.Version(\"3.6.4\"):\n",
        "            scores = [\n",
        "                meteor_score.single_meteor_score(\n",
        "                    word_tokenize(ref), word_tokenize(pred), alpha=alpha, beta=beta, gamma=gamma\n",
        "                )\n",
        "                for ref, pred in zip(references, predictions)\n",
        "            ]\n",
        "        else:\n",
        "            scores = [\n",
        "                meteor_score.single_meteor_score(ref, pred, alpha=alpha, beta=beta, gamma=gamma)\n",
        "                for ref, pred in zip(references, predictions)\n",
        "            ]\n",
        "\n",
        "        #return {\"meteor\": np.mean(scores)}\n",
        "        return np.mean(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka1NcTBvj4Zo",
        "outputId": "61fe8da5-ad83-4862-dfde-62b023bfb0c7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meteor Scores"
      ],
      "metadata": {
        "id": "6Qy9Ofyf4Jum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M = Meteor()\n",
        "#M._compute(\"How do you feedsadwd eweweqw wq ewql dsad\",\"How do you feel\")"
      ],
      "metadata": {
        "id": "i6UyNMP-j4z6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate scores...\n",
        "\n",
        "loop_sum_meteor = []\n",
        "for index in range(50):\n",
        "  score = M._compute(loop_sum[index],ref[index])\n",
        "  loop_sum_meteor.append(score)\n",
        "  print(score)\n",
        "\n",
        "match_sum_meteor = []\n",
        "for index in range(50):\n",
        "  score = M._compute(match_sum[index],ref[index])\n",
        "  match_sum_meteor.append(score)\n",
        "  #print(score)\n",
        "\n",
        "pgen_meteor = []\n",
        "for index in range(50):\n",
        "  score = M._compute(pgen[index],ref[index])\n",
        "  pgen_meteor.append(score)\n",
        "  #print(score)\n",
        "\n",
        "pnbert_meteor = []\n",
        "for index in range(50):\n",
        "  score = M._compute(pnbert[index],ref[index])\n",
        "  pnbert_meteor.append(score)\n",
        "  #print(score)"
      ],
      "metadata": {
        "id": "6cXi5YIH4JNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc5bbabb-7be6-4bab-ebb5-6a9984580c19"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.023346303501945526\n",
            "0.015209125475285171\n",
            "0.019650655021834062\n",
            "0.028846153846153848\n",
            "0.018779342723004695\n",
            "0.018018018018018018\n",
            "0.043668122270742356\n",
            "0.02830188679245283\n",
            "0.02511415525114155\n",
            "0.01327433628318584\n",
            "0.025943396226415096\n",
            "0.08982035928143713\n",
            "0.023529411764705882\n",
            "0.018092105263157895\n",
            "0.018145161290322582\n",
            "0.023148148148148147\n",
            "0.059322033898305086\n",
            "0.006276150627615063\n",
            "0.03145695364238411\n",
            "0.0371900826446281\n",
            "0.045454545454545456\n",
            "0.011061946902654867\n",
            "0.0625\n",
            "0.036101083032490974\n",
            "0.02534562211981567\n",
            "0.015544041450777202\n",
            "0.01936619718309859\n",
            "0.11923076923076924\n",
            "0.0641025641025641\n",
            "0.02459016393442623\n",
            "0.0589622641509434\n",
            "0.02891156462585034\n",
            "0.042600896860986545\n",
            "0.024734982332155476\n",
            "0.014545454545454545\n",
            "0.02850877192982456\n",
            "0.021400778210116732\n",
            "0.008620689655172414\n",
            "0.007246376811594203\n",
            "0.014830508474576272\n",
            "0.036585365853658534\n",
            "0.02181818181818182\n",
            "0.014285714285714285\n",
            "0.0525\n",
            "0.020491803278688523\n",
            "0.029801324503311258\n",
            "0.02108433734939759\n",
            "0.0532994923857868\n",
            "0.07954545454545454\n",
            "0.017241379310344827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write to files"
      ],
      "metadata": {
        "id": "4kzb0Eqq4nmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary of lists \n",
        "dict = {'loop_sum': loop_sum_meteor, 'match_sum': match_sum_meteor, 'pgen': pgen_meteor, 'pnbert':pnbert_meteor} \n",
        "     \n",
        "df = pd.DataFrame(dict)\n",
        "  \n",
        "# saving the dataframe\n",
        "df.to_csv('meteor_scores.csv')"
      ],
      "metadata": {
        "id": "j3tgu7K84pXb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FkOv0Twa4qNe"
      }
    }
  ]
}